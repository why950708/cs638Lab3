package hw3;

import java.util.*;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.io.BufferedReader;
import java.io.FileReader;
import javafx.util.Pair;
import hw3.Lab3;

/**
 * Do not modify.
 * 
 * This is the class with the main function
 */

public class oneLayer {
	/**
	 * Runs the tests for HW4
	 */

	Vector<Instance1> trainingSet;

	Vector<Instance1> testSet;

	Double[][] hiddenWeights;

	Double[][] outputWeights;

	Double learningRate;

	Integer[] outputs;

	NNImpl nn;

	public oneLayer(Vector<Vector<Double>> trainfeatureVectors, Vector<Vector<Double>> tunefeatureVectors,
			Vector<Vector<Double>> testfeatureVectors) {

		// Reading the training set
		trainingSet = getData(trainfeatureVectors);

		// Reading the weights
		hiddenWeights = new Double[Lab3.numberOfHiddenUnits][];

		for (int i = 0; i < hiddenWeights.length; i++) {
			hiddenWeights[i] = new Double[trainingSet.get(0).attributes.size() + 1];
		}

		outputWeights = new Double[trainingSet.get(0).classValues.size()][];
		for (int i = 0; i < outputWeights.length; i++) {
			outputWeights[i] = new Double[hiddenWeights.length + 1];
		}

		readWeights(hiddenWeights, outputWeights);

		learningRate = Lab3.eta;

		if (learningRate > 1 || learningRate <= 0) {
			System.out.println("Incorrect value for learning rate\n");
			System.exit(-1);
		}

		nn = new NNImpl(trainingSet, Lab3.numberOfHiddenUnits, learningRate, Lab3.maxEpochs, hiddenWeights,
				outputWeights);
		System.out.println(trainingSet.get(0).classValues.size());
		// nn.train();

		// Reading the testing set
		testSet = getData(testfeatureVectors);
		
		//Hongyi Wang for printing training set accuracy
		//outputs = new Integer[testSet.size()];
		outputs = new Integer[trainingSet.size()];

		
		//Hongyi Wang saved for later maybe
		// int correct = 0;
		// for (int i = 0; i < testSet.size(); i++) {
			//Getting output from network
			// outputs[i] = nn.calculateOutputForInstance(testSet.get(i));
			// int actual_idx = -1;
			// for (int j = 0; j < testSet.get(i).classValues.size(); j++) {
				// if (testSet.get(i).classValues.get(j) > 0.5)
					// actual_idx = j;
			// }

			// if (outputs[i] == actual_idx) {
				// correct++;
			// } else {
				// System.out.println(i + "th instance got an misclassification,
				// expected: " + actual_idx + ". But actual:" + outputs[i]);
			// }
		// }

		// System.out.println("Total instances: " + testSet.size());
		// System.out.println("Correctly classified: "+correct);

	}

	protected int printAccuracy() {
		int correct = 0;
		System.out.print(trainingSet.size());
		for (int i = 0; i < trainingSet.size(); i++) {
			// Getting output from network
			outputs[i] = nn.calculateOutputForInstance(trainingSet.get(i));
			int actual_idx = -1;
			for (int j = 0; j < trainingSet.get(i).classValues.size(); j++) {
				if (trainingSet.get(i).classValues.get(j) > 0.5)
					actual_idx = j;
			}

			if (outputs[i] == actual_idx) {
				correct++;
			} else {
				// System.out.println(i + "th instance got an misclassification,
				// expected: " + actual_idx + ". But actual:" + outputs[i]);
			}
		}
		System.out.println("Correct: " + correct);
		return correct;
	}

	// Reads a file and gets the list of instances
	private static Vector<Instance1> getData(Vector<Vector<Double>> featureVectors) {
		Vector<Instance1> data = new Vector<Instance1>();
		// Get attributes

		for (Vector<Double> featureVector : featureVectors) {
			Instance1 inst = new Instance1();
			for (Double attribute : featureVector) {
				// Add the class value
				if (attribute == featureVector.lastElement()) {
					for (int i = 0; i < 6; i++) {
						int classVal = (int) Math.round(attribute);
						if (i != (classVal))
							inst.classValues.add(0);
						else
							inst.classValues.add(1);
					}

				}
				// Add attributes into the instance list
				else
					inst.attributes.add(attribute);

			}
			data.add(inst);
		}
		return data;
	}

	// Gets weights randomly
	public static void readWeights(Double[][] hiddenWeights, Double[][] outputWeights) {
		// Use the given random weight generator in Lab3
		 Random r = new Random();
		for (int i = 0; i < hiddenWeights.length; i++) {
			for (int j = 0; j < hiddenWeights[i].length; j++) {
				// Fan in for the hidden layer is the number of inputs, fan out
				// is the num of outputs
				//hiddenWeights[i][j] = Lab3.getRandomWeight(Lab3.inputVectorSize, outputWeights.length);
				hiddenWeights[i][j] = r.nextDouble()*0.01;
			}
		}

		for (int i = 0; i < outputWeights.length; i++) {
			for (int j = 0; j < outputWeights[i].length; j++) {
				// Fan in for the output layer is the number of hidden layer,
				// fan out is 1
				//outputWeights[i][j] = Lab3.getRandomWeight(hiddenWeights.length, 1);
				outputWeights[i][j] = r.nextDouble()*0.01;
			}

			// The original method
			// Random r = new Random();

			// for(int i=0;i<hiddenWeights.length;i++)
			// {
			// for(int j=0;j<hiddenWeights[i].length;j++)
			// {
			// hiddenWeights[i][j] = r.nextDouble()*0.01;
			// }
			// }

			// for(int i=0;i<outputWeights.length;i++)
			// {
			// for (int j=0; j<outputWeights[i].length; j++)
			// {
			// outputWeights[i][j] = r.nextDouble()*0.01;
			// }
			// }

			// }
		}
	}
}

/**
 * The main class that handles the entire network Has multiple attributes each
 * with its own use
 * 
 */

class NNImpl {
	public Vector<Node> inputNodes = null;// list of the output layer nodes.
	public Vector<Node> hiddenNodes = null;// list of the hidden layer nodes
	public Vector<Node> outputNodes = null;// list of the output layer nodes

	public Vector<Instance1> trainingSet = null;// the training set
	Double[][] hiddenWeightss;
	Double[][] outputWeightss;
	Double learningRate = 1.0; // variable to store the learning rate
	int maxEpoch = 1; // variable to store the maximum number of epochs

	/**
	 * This constructor creates the nodes necessary for the neural network Also
	 * connects the nodes of different layers After calling the constructor the
	 * last node of both inputNodes and hiddenNodes will be bias nodes.
	 */

	public NNImpl(Vector<Instance1> trainingSet, int hiddenNodeCount, Double learningRate, int maxEpoch,
			Double[][] hiddenWeights, Double[][] outputWeights) {

		this.trainingSet = trainingSet;
		this.learningRate = learningRate;
		this.maxEpoch = maxEpoch;
		this.hiddenWeightss = hiddenWeights;
		this.outputWeightss = outputWeights;
		// input layer nodes
		inputNodes = new Vector<Node>();
		int inputNodeCount = trainingSet.get(0).attributes.size();
		int outputNodeCount = trainingSet.get(0).classValues.size();
		for (int i = 0; i < inputNodeCount; i++) {
			Node node = new Node(0);
			inputNodes.add(node);
		}

		// bias node from input layer to hidden
		Node biasToHidden = new Node(1);
		inputNodes.add(biasToHidden);

		// hidden layer nodes
		hiddenNodes = new Vector<Node>();
		for (int i = 0; i < hiddenNodeCount; i++) {
			Node node = new Node(2);
			// Connecting hidden layer nodes with input layer nodes
			for (int j = 0; j < inputNodes.size(); j++) {
				NodeWeightPair nwp = new NodeWeightPair(inputNodes.get(j), hiddenWeights[i][j]);
				node.parents.add(nwp);
			}
			hiddenNodes.add(node);
		}

		// bias node from hidden layer to output
		Node biasToOutput = new Node(3);
		hiddenNodes.add(biasToOutput);

		// Output node layer
		outputNodes = new Vector<Node>();
		for (int i = 0; i < outputNodeCount; i++) {
			Node node = new Node(4);
			// Connecting output layer nodes with hidden layer nodes
			for (int j = 0; j < hiddenNodes.size(); j++) {
				NodeWeightPair nwp = new NodeWeightPair(hiddenNodes.get(j), outputWeights[i][j]);
				node.parents.add(nwp);
			}
			outputNodes.add(node);
		}
	}

	/**
	 * Get the output from the neural network for a single instance Return the
	 * idx with highest output values. For example if the outputs of the
	 * outputNodes are [0.1, 0.5, 0.2], it should return 1. If outputs of the
	 * outputNodes are [0.1, 0.5, 0.5], it should return 2. The parameter is a
	 * single instance.
	 */

	public int calculateOutputForInstance(Instance1 inst) {
		// TODO: add code here
		// Get the input from the instance
		for (int i = 0; i < inst.attributes.size(); i++) {
			this.inputNodes.get(i).setInput(inst.attributes.get(i));
			// Hongyi Wang debugging
			// if(inputNodes.get(i).getOutput()>1)
			// System.out.print("intput value: " +
			// inputNodes.get(i).getOutput());
		}

		for (Node hiddenNode : this.hiddenNodes) {
			hiddenNode.calculateOutput();
		}

		for (Node outputNode : this.outputNodes) {
			outputNode.calculateOutput();
		}

		double max = 0;
		int index = 0;

		// Hongyi Wang try to print output for hidden nodes
		for (int i = 0; i < this.inputNodes.size(); i++) {
			Node hidden = this.inputNodes.get(i);
			// System.out.print(" hidden output: " + hidden.getOutput() + " ");
			// System.out.println(" hidden input: " + hidden.getInput() + " ");
		}

		for (int i = 0; i < this.outputNodes.size(); i++) {
			Node output = this.outputNodes.get(i);
			// Hongyi Wang debugging
			// System.out.println(" ouput input: " + output.getInput() + " ");
			if (output.getOutput() > max) {
				max = output.getOutput();
				index = i;
			}
		}
		return index;

	}

	/**
	 * Train the neural networks with the given parameters
	 * 
	 * The parameters are stored as attributes of this class
	 */

	public void train() {
		double sum = 0;
		// TODO: add code here
		for (int i = 0; i < maxEpoch; i++) {

			for (Instance1 inst : this.trainingSet) {
				this.calculateOutputForInstance(inst); // forward
				// Backward
				for (int j = 0; j < this.outputNodes.size(); j++) {
					int derivative = (outputNodes.get(j).getOutput() > 0) ? 1 : 0;
					double TO = inst.classValues.get(j) - outputNodes.get(j).getOutput();
					// Compute Wjk

					for (NodeWeightPair pair : outputNodes.get(j).parents) {
						double weight = this.learningRate * pair.node.getOutput() * derivative * TO;
						pair.weight += weight;
					}

				}
				// Compute Wij
				for (int j = 0; j < this.hiddenNodes.size(); j++) {
					double hiddenDerivative = (hiddenNodes.get(j).getSum() > 0) ? 1 : 0;
					if (hiddenNodes.get(j).parents == null) {
						continue;

					}
					for (NodeWeightPair pair : hiddenNodes.get(j).parents) {
						double total = 0;
						for (Node output : this.outputNodes) {
							double TO = inst.classValues.get(this.outputNodes.indexOf(output)) - output.getOutput();
							int derivative = (output.getSum() > 0) ? 1 : 0;
							total += derivative * TO * getHiddenPairWeight(this.hiddenNodes.get(j), output).weight;
						}
						Node input = pair.node;
						double weight = this.learningRate * input.getOutput() * hiddenDerivative * total;
						pair.weight += weight;
					}
				}
			}
		}
	}
	
	
	
	
	
	
	
	
	
	

	// Train teh neural network for one epochs

	public void trainOneEpoch() {
		//Randomnize the set to prevent overfitting
		Lab3.permute(trainingSet);
		double sum = 0;
		ExecutorService executor = Executors.newCachedThreadPool();
		for (Instance1 inst : this.trainingSet) {
			this.calculateOutputForInstance(inst); // forward
			// Backward
			
			for (int j = 0; j < this.outputNodes.size(); j++) {
				// Hongyi Wang debugging
				// System.out.println("Actual output: " +
				// outputNodes.get(j).getOutput()+ " number: "+ j);
				int derivative = (outputNodes.get(j).getOutput() >= 0) ? 1 : 0;
				double error = inst.classValues.get(j) - outputNodes.get(j).getOutput();

				
				// Compute Wjk
				
				executor.execute(new tryThreads(j,derivative,error, outputNodes, learningRate));
				
				

			}
			// Compute Wij
			for (int j = 0; j < this.hiddenNodes.size(); j++) {
				double hiddenDerivative = (hiddenNodes.get(j).getSum() >= 0) ? 1 : 0;
				if (hiddenNodes.get(j).parents == null) {
					continue;

				}
				for (NodeWeightPair pair : hiddenNodes.get(j).parents) {
					double total = 0;
					for (Node output : this.outputNodes) {
						double error = inst.classValues.get(this.outputNodes.indexOf(output)) - output.getOutput();
						double derivativeSigmoid = (output.getOutput()*(1-output.getOutput()));
						total += derivativeSigmoid * error * getHiddenPairWeight(this.hiddenNodes.get(j), output).weight;
					}
					Node input = pair.node;
					double weight = this.learningRate * input.getOutput() * hiddenDerivative * total;
					pair.weight += weight;
					//// Hongyi Wang to print the node 0 weight to check if
					//// anything is changing
					// NodeWeightPair pair1 = hiddenNodes.get(0).parents.get(0);
					// {

					// System.out.println("Weight for 0 node: "+ pair1.weight);
					// System.out.println("Change of weight: "+weight);
					// }
				}
			}

		}

		// Hongyi Wang print output weightss
		// for(int i = 0; i<outputWeightss.length;i++)
		// {
		// for(int j =0;j<outputWeightss[0].length;j++)
		// {
		// System.out.print(outputWeightss[i][j]+" ");
		// }
		// System.out.println();
		// }
		// System.out.println("here!");

	}

	private NodeWeightPair getHiddenPairWeight(Node nodeIn, Node output) {
		// TODO Auto-generated method stub
		for (NodeWeightPair pair : output.parents) {
			if (pair.node.equals(nodeIn))
				return pair;
		}
		return null;
	}
}

/**
 * Class for internal organization of a Neural Network. There are 5 types of
 * nodes. Check the type attribute of the node for details
 * 
 * Do not modify.
 */

class Node {
	private int type = 0; // 0=input,1=biasToHidden,2=hidden,3=biasToOutput,4=Output
	public Vector<NodeWeightPair> parents = null; // Array List that will
													// contain the parents
													// (including the bias node)
													// with weights if
													// applicable

	private Double inputValue = 0.0;
	private Double outputValue = 0.0;
	private Double sum = 0.0; // sum of wi*xi

	// Create a node with a specific type
	public Node(int type) {
		if (type > 4 || type < 0) {
			System.out.println("Incorrect value for node type");
			System.exit(1);

		} else {
			this.type = type;
		}

		if (type == 2 || type == 4) {
			parents = new Vector<NodeWeightPair>();
		}
	}

	// For an input node sets the input value which will be the value of a
	// particular attribute
	public void setInput(Double inputValue) {
		if (type == 0)// If input node
		{
			this.inputValue = inputValue;
		}
	}

	// Hongyi Wang for debugging get input
	public double getInput() {
		return inputValue;
	}

	/**
	 * Calculate the output of a ReLU node. You can assume that outputs of the
	 * parent nodes have already been calculated You can get this value by using
	 * getOutput()
	 * 
	 * @param train:
	 *            the training set
	 */
	public void calculateOutput() {

		if (type == 2 || type == 4)// Not an input or bias node
		{
			// TODO: add code here
			double cache = 0;
			for (NodeWeightPair nwp : this.parents) {
				// Hongyi Wang print weight for debugging
				// if(type == 2) System.out.println(" weight"
				// +nwp.node.getOutput() + " ");
				cache += nwp.node.getOutput() * nwp.weight;

			}
			this.sum = cache;

			if (cache < 0 && type == 2)
				cache = 0;
			//Sigmoid for output too prevent weight explosion
			if (type == 4)
				cache = sigmoid(cache);
			
			this.outputValue = cache;
		}
	}
	
	public double sigmoid(double x)
	{
		return 1.0/(1+Math.exp(-x));
	}
	
	public double getSum() {
		return sum;
	}

	// Gets the output value
	public double getOutput() {

		if (type == 0)// Input node
		{
			return inputValue;
		} else if (type == 1 || type == 3)// Bias node
		{
			return 1.00;
		} else {
			return outputValue;
		}

	}
}

/**
 * Class to identfiy connections between different layers.
 * 
 */

class NodeWeightPair {
	public Node node; // The parent node
	public Double weight; // Weight of this connection
	public Double oldWeight;

	// Create an object with a given parent node
	// and connect weight
	public NodeWeightPair(Node node, Double weight) {
		this.node = node;
		this.weight = weight;
		this.oldWeight = weight;
	}

}

/**
 * Holds data for a particular instance. Attributes are represented as an Vector
 * of Doubles Class labels are represented as an Vector of Integers. For
 * example, a 3-class instance will have classValues as [0 1 0] meaning this
 * instance has class 1. Do not modify
 */

class Instance1 {
	public Vector<Double> attributes;
	public Vector<Integer> classValues;

	public Instance1() {
		attributes = new Vector<Double>();
		classValues = new Vector<Integer>();
	}

}

//Hongyi Wang utilizing threads try to speed up the program on multicore system
	class tryThreads  implements Runnable 
	{
		int j;
		int derivative;
		double error;
		Vector<Node> outputNodes;
		double learningRate;
		public tryThreads (int j, int derivative, double error, Vector<Node> outputNodes, double learningRate )
		{
			this.j = j;
			this.derivative = derivative;
			this.error = error;
			this.outputNodes = outputNodes;
			this.learningRate = learningRate;
		}
		public void run()
		{
			for (NodeWeightPair pair : outputNodes.get(j).parents) {
					double weight = learningRate * pair.node.getOutput() * derivative * error;
					pair.weight += weight;	
				}
		}
	}
